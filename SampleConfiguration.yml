batch_size: 23 
constant_warmup: false                # Uses the get_constant_schedule_with_warmup function from HuggingFace with a step size of 10 for warmup adjusting each epoch
image_height: 384                     # This height will be used by the pre-processor to format the image correctly for the model
image_width: 384
linear_schedule_with_warmup: false    # Uses the get_linear_schedule_with_warmup function from HuggingFace with a step size of 10 and decreasing until the last epoch
lr: 5.0e-05
max_tokens: 50                        # The maximum number of tokens to be generated by the model
model_pretrained_path: microsoft/trocr-small-handwritten # The path of the pretrained model, can be a local directory w/ config, gen_config, and bin files or a HF repository
num_epochs: 50                        # The number of epochs to trian for 
num_images: null                      # The number of images to be used from the training data, useful for overfitting to a single image or for running tests with a subset of the images
num_workers: 9                        # The number of workers used by the DataLoader
print_eval_cer_every: 5               # How often (epochs) to print the evaluation CER during training 
print_loss_every: 1                   # How often (epochs) to print the loss during training (requires train=True)
print_num_samples: 1                  # When printing the evaluation or training CER, this allows you to print a sample of the data points being evaluated
print_train_cer_every: 5              # How often (epochs) to print the training CER during training  
processor_pretrained_path: microsoft/trocr-small-handwritten # The path of the pretrained processor
root_directory: /home/jclar234/TrOCR  # The root directory of the repository, used to find images
save_directory: null                  # The directory to save an intermediate/end model to during training (requires train=True)
save_every: 5                         # How often (epochs) to save the model to the save directory (requires train=True, save_directory!=null)
tokenizer_type: null                  # modifies the processor's tokenizer, valid values include BERT or CANINE or NULL
train: true                           # Whether the model is being used for evaluation or training
train_only_embeddings: false          # Disables the gradient on all layers except the learned embeddings of the model
unfreeze: false                       # Gradually unfreezes layers of the model starting from the embedding layer to the output layer
use_double: false                     # Uses the IAMDatasetTwoLines for training images
use_learned_position_embeddings: true # Whether to use learned positional embeddings or sinusoidal positional embeddings
